{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Objective:\n",
    "To develop a Spark Streaming application that reads JSON data from a Kafka topic,\n",
    "processes the data, and writes the results into the S3 in Parquet format.\n",
    "\n",
    "Overview:\n",
    "This Spark Streaming application is designed to integrate with Apache Kafka for data\n",
    "ingestion. It reads JSON-formatted review data associated with applications, parses\n",
    "this data, and subsequently stores it on S3 for further analysis or processing. The\n",
    "primary workflow includes initializing a Spark session, defining the data schema,\n",
    "streaming data from Kafka, parsing the JSON data, and persisting the processed data\n",
    "to S3.\n",
    "'''\n",
    "\n",
    "# Spark Environment Setup: Import Libraries:\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the JSON Schema for Incoming Data:\n",
    "\n",
    "'''\n",
    "Define the JSON Schema for Incoming Data:\n",
    "In Apache Spark, when dealing with structured data (especially data that comes in\n",
    "structured formats like JSON), it is often necessary to define the schema to inform\n",
    "Spark about the structure of the data. This helps in efficient processing and in\n",
    "ensuring data integrity.\n",
    "In the given code, we're defining a schema for the expected JSON data. The schema\n",
    "is defined using Spark's StructType and StructField classes from the\n",
    "pyspark.sql.types module.\n",
    "'''\n",
    "\n",
    "json_schema = T.StructType([\n",
    "T.StructField('application_name', T.StringType()),\n",
    "T.StructField('num_of_positive_sentiments', T.LongType()),\n",
    "T.StructField('num_of_neutral_sentiments', T.LongType()),\n",
    "T.StructField('num_of_negative_sentiments', T.LongType()),\n",
    "T.StructField('avg_sentiment_polarity', T.DoubleType()),\n",
    "T.StructField('avg_sentiment_subjectivity', T.DoubleType()),\n",
    "T.StructField('category', T.StringType()),\n",
    "T.StructField('rating', T.StringType()),\n",
    "T.StructField('reviews', T.StringType()),\n",
    "T.StructField('size', T.StringType()),\n",
    "T.StructField('num_of_installs', T.DoubleType()),\n",
    "T.StructField('price', T.DoubleType()),\n",
    "T.StructField('age_limit', T.LongType()),\n",
    "T.StructField('genres', T.StringType()),\n",
    "T.StructField('version', T.StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Initialize Spark Session: Establish a local Spark session utilizing all available cores and\n",
    "adding a Spark-Kafka integration package, which allows Spark to interact with Kafka.\n",
    "'''\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".master(\"local\") \\\n",
    ".appName('ex6_store_results') \\\n",
    ".config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Streaming Data from Kafka\n",
    "'''\n",
    "initializes a streaming DataFrame (stream_df) from a Kafka source. Here's a brief\n",
    "explanation:\n",
    "    • sets up a streaming read from Kafka.\n",
    "    • Specifies the Kafka bootstrap server address as \"course-kafka:9092\".\n",
    "    • Subscribes to the Kafka topic \"gps-with-reviews\".\n",
    "    • Sets the starting offsets to the earliest, meaning it will process data from the\n",
    "      beginning of the topic.\n",
    "    • Finally, after loading the data, it selects the 'value' column and casts it to a\n",
    "    string type.\n",
    "'''\n",
    "\n",
    "\n",
    "stream_df = spark \\\n",
    ".readStream \\\n",
    ".format('kafka') \\\n",
    ".option(\"kafka.bootstrap.servers\", \"course-kafka:9092\") \\\n",
    ".option(\"subscribe\", \"gps-with-reviews\") \\\n",
    ".option('startingOffsets', 'earliest') \\\n",
    ".load() \\\n",
    ".select(F.col('value').cast(T.StringType()))\n",
    "\n",
    "'''\n",
    "Today's tip\n",
    "    In Kafka, each message consists of a key and a value. When you consume\n",
    "    messages from Kafka using Spark Structured Streaming, these messages are read\n",
    "    into a DataFrame with multiple columns, two of which are key and value.\n",
    "    The value column in this context contains the actual content of the Kafka message.\n",
    "    In the provided code, the value column is being selected and cast to a string type.\n",
    "    This implies that the actual message content resides in the value column, and it's\n",
    "    being treated as a string (which is often the case when dealing with JSON-\n",
    "    formatted messages in Kafka).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nF.col(\\'parsed_json.*\\'): This syntax is used to select all the individual fields from the\\nstructured column parsed_json and elevate them to the top level. Essentially, it\\n\"flattens\" the nested or structured data in parsed_json\\n\\n# chatGBT:\\nF.col(\\'parsed_json.*\\'): This expands the fields inside the parsed_json column into individual columns.\\nThe .* syntax allows Spark to take all the fields from the StructType (parsed_json) and create separate columns for each of the fields.\\n\\nWhat happens here:\\n\\nAfter parsing the JSON into the parsed_json column (which is a StructType),\\nthe select statement \"explodes\" the fields inside parsed_json into separate top-level columns.\\nFor example, given the schema:\\n\\njson_schema = T.StructType([\\n    T.StructField(\\'application_name\\', T.StringType()),\\n    T.StructField(\\'translated_review\\', T.StringType()),\\n    T.StructField(\\'sentiment_rank\\', T.IntegerType()),\\n    T.StructField(\\'sentiment_polarity\\', T.FloatType()),\\n    T.StructField(\\'sentiment_subjectivity\\', T.FloatType())\\n])\\n\\nThe resulting DataFrame will have individual columns for each field in the JSON:\\n\\nThe resulting DataFrame will have individual columns for each field in the JSON:\\n\\napplication_name\\ttranslated_review\\tsentiment_rank\\tsentiment_polarity\\tsentiment_subjectivity\\nApp A\\t                Good app\\t        1\\t            0.8\\t                0.6\\nApp B\\tN               eeds improvement\\t2\\t            -0.5\\t            0.9\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse the JSON content\n",
    "\n",
    "#First Part:\n",
    "parsed_df = stream_df \\\n",
    ".withColumn('parsed_json', F.from_json(F.col('value'), json_schema))\n",
    "\n",
    "'''\n",
    "    • F.from_json(...): is a PySpark SQL function that parses a column of JSON strings\n",
    "        into a structured format based on the provided schema.\n",
    "    • It fetches the column named value from the DataFrame. This column contains\n",
    "        the JSON strings and apply the provided schema\n",
    "    • After this operation, the DataFrame will contain a new column named\n",
    "        parsed_json. This new column holds the structured data that results from parsing\n",
    "        the JSON strings in the value column.\n",
    "# ChatGBT\n",
    "1. Original Column (value):\n",
    "    The value column contains the JSON data as plain text (strings) from Kafka.\n",
    "    It looks like this in its raw form:\n",
    "    value\n",
    "    {\"application_name\": \"App A\", \"translated_review\": \"Good app\", \"sentiment_rank\": 1, ...}\n",
    "\n",
    "    Type: StringType (It's just a long string, unstructured).\n",
    "    Content: Raw JSON as a string.\n",
    "\n",
    "In this form, Spark does not yet \"understand\" or recognize that it's a JSON object.\n",
    "It's just a block of text, and you can't easily access individual fields like application_name or sentiment_polarity.\n",
    "\n",
    "2. New Column (parsed_json):\n",
    "    When you apply F.from_json(F.col('value'), json_schema),\n",
    "    you are transforming the raw JSON string into a structured format according to the schema (json_schema), \n",
    "    so Spark can recognize each part of the JSON as a structured object.\n",
    "\n",
    "After parsing, the new column parsed_json contains a StructType, which is like a nested data structure.\n",
    "Each field inside the JSON becomes an individual part of this structure.\n",
    "\n",
    "The column looks similar to this:\n",
    "parsed_json\n",
    "{\"application_name\": \"App A\", \"translated_review\": \"Good app\", \"sentiment_rank\": 1, ...}\n",
    "\n",
    "But internally, Spark knows this is no longer just a string, but a structured object with individual fields that match the schema you provided.\n",
    "\n",
    "Type: StructType (A structured object with fields like application_name, sentiment_rank, etc.).\n",
    "Content: A parsed JSON object with fields recognized by Spark based on the schema.\n",
    "\n",
    "This structure allows you to access each field individually and perform operations on them.\n",
    "\n",
    "Difference Summary:\n",
    "Original column (value): This is a plain string. You can't easily access individual parts of the JSON without parsing it.\n",
    "New column (parsed_json): This is a structured object (StructType). Spark recognizes the individual fields inside the JSON, and now you can easily work with those fields in subsequent transformations.\n",
    "Why Is This Important?\n",
    "The structured nature of parsed_json allows you to access and manipulate each part of the JSON.\n",
    "'''\n",
    "\n",
    "#Second part\n",
    "\n",
    "parsed_df = parsed_df.select(F.col('parsed_json.*'))\n",
    "\n",
    "'''\n",
    "F.col('parsed_json.*'): This syntax is used to select all the individual fields from the\n",
    "structured column parsed_json and elevate them to the top level. Essentially, it\n",
    "\"flattens\" the nested or structured data in parsed_json\n",
    "\n",
    "# chatGBT:\n",
    "F.col('parsed_json.*'): This expands the fields inside the parsed_json column into individual columns.\n",
    "The .* syntax allows Spark to take all the fields from the StructType (parsed_json) and create separate columns for each of the fields.\n",
    "\n",
    "What happens here:\n",
    "\n",
    "After parsing the JSON into the parsed_json column (which is a StructType),\n",
    "the select statement \"explodes\" the fields inside parsed_json into separate top-level columns.\n",
    "For example, given the schema:\n",
    "\n",
    "json_schema = T.StructType([\n",
    "    T.StructField('application_name', T.StringType()),\n",
    "    T.StructField('translated_review', T.StringType()),\n",
    "    T.StructField('sentiment_rank', T.IntegerType()),\n",
    "    T.StructField('sentiment_polarity', T.FloatType()),\n",
    "    T.StructField('sentiment_subjectivity', T.FloatType())\n",
    "])\n",
    "\n",
    "The resulting DataFrame will have individual columns for each field in the JSON:\n",
    "\n",
    "The resulting DataFrame will have individual columns for each field in the JSON:\n",
    "\n",
    "application_name\ttranslated_review\tsentiment_rank\tsentiment_polarity\tsentiment_subjectivity\n",
    "App A\t                Good app\t        1\t            0.8\t                0.6\n",
    "App B\tN               eeds improvement\t2\t            -0.5\t            0.9\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/26 08:07:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/09/26 08:07:02 WARN StreamingQueryManager: Stopping existing streaming query [id=be405fe6-268b-433d-8553-79f9fbaa369d, runId=fdf9412a-78df-4d32-9fc2-2b8788312f14], as a new run is being started.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n• parsed_df.writeStream:\\n    This code is preparing to write the data contained in parsed_df as a stream. PySpark\\n    provides stream processing capabilities, and this is the starting point for setting up\\n    the streaming write.\\n• .trigger(processingTime=\\'1 minute\\'):\\n    This sets the trigger for the streaming query to process data once every minute.\\n• .format(\\'parquet\\'):\\n    This specifies that the output data format should be parquet, which is a popular\\n    columnar storage format.\\n• .outputMode(\\'append\\'):\\n    This indicates that new data should be appended to the result table/directory.\\n\\n• .option(\"path\", \"s3a://spark/data/target/google_reviews_calc\"):\\n    This sets the destination where the processed streaming data should be written.\\n    Here, it is specifying a location in the S3.\\n• .option(\\'checkpointLocation\\', \\'s3a://spark/checkpoints/ex6/store_result\\'):\\n    Streaming queries in PySpark need a location to store recovery information, which is\\n    used in case of a failure to restart the stream from where it left off. This is called a\\n    checkpoint location, and it\\'s set to a directory in S3 in this code.\\n• .start():\\n    This method actually starts the streaming query. Until this method is called, no data\\n    is processed or written. Once started, the data will be processed according to the\\n    configurations set up above and written to the specified location in S3.\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/26 08:07:02 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "24/09/26 08:07:02 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "24/09/26 08:07:02 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "24/09/26 08:07:02 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "24/09/26 08:07:02 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n"
     ]
    }
   ],
   "source": [
    "# write the data as a stream to S3\n",
    "\n",
    "query = parsed_df \\\n",
    ".writeStream \\\n",
    ".trigger(processingTime='1 minute') \\\n",
    ".format('parquet') \\\n",
    ".outputMode('append') \\\n",
    ".option(\"path\", \"s3a://spark/data/target/google_reviews_calc\") \\\n",
    ".option('checkpointLocation', 's3a://spark/checkpoints/ex6/store_result') \\\n",
    ".start()\n",
    "\n",
    "'''\n",
    "\n",
    "• parsed_df.writeStream:\n",
    "    This code is preparing to write the data contained in parsed_df as a stream. PySpark\n",
    "    provides stream processing capabilities, and this is the starting point for setting up\n",
    "    the streaming write.\n",
    "• .trigger(processingTime='1 minute'):\n",
    "    This sets the trigger for the streaming query to process data once every minute.\n",
    "• .format('parquet'):\n",
    "    This specifies that the output data format should be parquet, which is a popular\n",
    "    columnar storage format.\n",
    "• .outputMode('append'):\n",
    "    This indicates that new data should be appended to the result table/directory.\n",
    "\n",
    "• .option(\"path\", \"s3a://spark/data/target/google_reviews_calc\"):\n",
    "    This sets the destination where the processed streaming data should be written.\n",
    "    Here, it is specifying a location in the S3.\n",
    "• .option('checkpointLocation', 's3a://spark/checkpoints/ex6/store_result'):\n",
    "    Streaming queries in PySpark need a location to store recovery information, which is\n",
    "    used in case of a failure to restart the stream from where it left off. This is called a\n",
    "    checkpoint location, and it's set to a directory in S3 in this code.\n",
    "• .start():\n",
    "    This method actually starts the streaming query. Until this method is called, no data\n",
    "    is processed or written. Once started, the data will be processed according to the\n",
    "    configurations set up above and written to the specified location in S3.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# streaming query termination\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m query\u001b[39m.\u001b[39;49mawaitTermination()\n\u001b[1;32m      4\u001b[0m spark\u001b[39m.\u001b[39mstop()\n\u001b[1;32m      6\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mAfter starting the streaming query, this line of code will make the application block\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mand wait for the streaming query to terminate, either due to a failure or a manual\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[39m'''\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/query.py:201\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsq\u001b[39m.\u001b[39mawaitTermination(\u001b[39mint\u001b[39m(timeout \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m))\n\u001b[1;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsq\u001b[39m.\u001b[39;49mawaitTermination()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# streaming query termination\n",
    "\n",
    "query.awaitTermination()\n",
    "spark.stop()\n",
    "\n",
    "'''\n",
    "After starting the streaming query, this line of code will make the application block\n",
    "and wait for the streaming query to terminate, either due to a failure or a manual\n",
    "termination. It's essentially saying, \"Hold on here and keep running until the\n",
    "streaming process finishes for some reason.\"\n",
    "Finally, after the streaming query terminates (or in some setups, potentially never if\n",
    "the awaitTermination() doesn't detect a termination), spark.stop() line stops the\n",
    "SparkSession, freeing up any resources and ending the application.\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
