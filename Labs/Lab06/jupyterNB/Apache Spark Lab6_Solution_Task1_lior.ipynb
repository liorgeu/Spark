{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Here, we're importing necessary modules and setting the correct environment paths for\\n    PySpark, Hadoop, and Python.\\n\\n\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 1: Imports and Environment Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "'''\n",
    "    Here, we're importing necessary modules and setting the correct environment paths for\n",
    "    PySpark, Hadoop, and Python.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nDefines the structure (json_schema) for parsing the incoming JSON data. This structure\\ndictates the expected fields in the JSON and their corresponding data types.\\n\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 2: Define JSON Schema for Incoming Data\n",
    "json_schema = T.StructType([\n",
    "T.StructField('application_name', T.StringType()),\n",
    "T.StructField('translated_review', T.StringType()),\n",
    "T.StructField('sentiment_rank', T.IntegerType()),\n",
    "T.StructField('sentiment_polarity', T.FloatType()),\n",
    "T.StructField('sentiment_subjectivity', T.FloatType())\n",
    "])\n",
    "\n",
    "''' \n",
    "Defines the structure (json_schema) for parsing the incoming JSON data. This structure\n",
    "dictates the expected fields in the JSON and their corresponding data types.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n    Initializes a SparkSession which is the entry point to any Spark functionality.\\n    Sets the application name and the master for the SparkSession.\\n    Adds a specific jar for Kafka integration to the Spark job.\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 3: Initialize Spark Session\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".master(\"local\") \\\n",
    ".appName('ex6_calculate_reviews') \\\n",
    ".config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2') \\\n",
    ".getOrCreate()\n",
    "\n",
    "''' \n",
    "    Initializes a SparkSession which is the entry point to any Spark functionality.\n",
    "    Sets the application name and the master for the SparkSession.\n",
    "    Adds a specific jar for Kafka integration to the Spark job.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\n# Start streaming and print to the console\\nquery = stream_df     .writeStream     .outputMode(\"append\")     .format(\"console\")     .option(\"truncate\", \"false\").start()\\n\\n# Let the stream run for a specific time (e.g., 10 seconds) instead of awaitTermination()\\nimport time\\ntime.sleep(10)  # Run the stream for 10 seconds (adjust as needed)\\n\\n# Stop the query manually\\nquery.stop()\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 4: Consume Data from Kafka\n",
    "\n",
    "'''\n",
    "    Reads streaming data from the Kafka topic named \"gps-user-review-source\").\n",
    "    The incoming byte data from Kafka is then converted to a string.\n",
    "\n",
    "'''\n",
    "\n",
    "stream_df = spark \\\n",
    ".readStream \\\n",
    ".format('kafka') \\\n",
    ".option(\"kafka.bootstrap.servers\", \"course-kafka:9092\") \\\n",
    ".option(\"subscribe\", \"gps-user-review-source\") \\\n",
    ".option('startingOffsets', 'earliest') \\\n",
    ".load() \\\n",
    ".select(F.col('value').cast(T.StringType()))\n",
    "\n",
    "'''  \n",
    "# Start streaming and print to the console\n",
    "query = stream_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\").start()\n",
    "\n",
    "# Let the stream run for a specific time (e.g., 10 seconds) instead of awaitTermination()\n",
    "import time\n",
    "time.sleep(10)  # Run the stream for 10 seconds (adjust as needed)\n",
    "\n",
    "# Stop the query manually\n",
    "query.stop()\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nparsed_df = stream_df .withColumn('parsed_json', F.from_json(F.col('value'), json_schema)) .select(F.col('parsed_json.*'))\\n\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 5: Parse the JSON Data\n",
    "parsed_df = stream_df \\\n",
    ".withColumn('parsed_json', F.from_json(F.col('value'), json_schema)) \\\n",
    ".select(F.col('parsed_json.*'))\n",
    "\n",
    "\n",
    "'''\n",
    "Original column (value): This is a plain string. You can't easily access individual parts of the JSON without parsing it.\n",
    "New column (parsed_json): This is a structured object (StructType). Spark recognizes the individual fields inside the JSON,\n",
    "and now you can easily work with those fields in subsequent transformations.\n",
    "\n",
    "Why Is This Important?\n",
    "The structured nature of parsed_json allows you to access and manipulate each part of the JSON. For example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "parsed_df.select(\"parsed_json.application_name\", \"parsed_json.sentiment_rank\").show()\n",
    "Will give you:\n",
    "\n",
    "application_name\tsentiment_rank\n",
    "App A\t                1\n",
    "App B\t                2\n",
    "This would not be possible if the data was still in the unparsed value column because it is just a string.\n",
    "\n",
    "This transformation is what allows you to access individual elements of the JSON and treat them like regular columns in Spark.\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "parsed_df = stream_df \\\n",
    ".withColumn('parsed_json', F.from_json(F.col('value'), json_schema)) \\\n",
    ".select(F.col('parsed_json.*'))\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Print stream_df to the console\n",
    "query_stream_df = stream_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Print parsed_df to the console\n",
    "query_parsed_df = parsed_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the streaming queries to finish (or you can specify a timeout)\n",
    "query_stream_df.awaitTermination()\n",
    "query_parsed_df.awaitTermination()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/26 05:41:40 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[application_name: string, category: string, rating: string, reviews: float, size: string, num_of_installs: double, price: double, age_limit: bigint, genres: string, version: string]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 6: Load Static Data from Parquet and Cache\n",
    "\n",
    "static_data_df = spark.read.parquet('s3a://spark/data/source/google_apps/')\n",
    "static_data_df.cache()\n",
    "\n",
    "'''  \n",
    "Reads static data stored in Parquet format.\n",
    "Caches (in-memory storage) the data for better performance in subsequent operations.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Aggregation, Transformation, and Join Operations\n",
    "\n",
    "joined_df = parsed_df \\\n",
    ".groupBy(F.col('application_name'))\\\n",
    ".agg(F.sum(F.when(F.col('sentiment_rank') == 1, 1).otherwise(0)).alias('num_of_positive_sentiments'),\n",
    "    F.sum (F.when (F.col('sentiment_rank') == 0, 1).otherwise(0)).alias('num_of_neutral_sentiments'),\n",
    "    F.sum(F.when (F.col('sentiment_rank') == -1, 1).otherwise(0)).alias('num_of_negative_sentiments'),\n",
    "    F.avg(F.col('sentiment_polarity')).alias('avg_sentiment_polarity'),\n",
    "    F.avg(F.col('sentiment_subjectivity')).alias('avg_sentiment_subjectivity'))\\\n",
    ".join(static_data_df, ['application_name'])\n",
    "\n",
    "# Joining with Static Data:\n",
    "\n",
    "'''\n",
    ".join(static_data_df, ['application_name'])\n",
    "Explanation: Once the aggregation is done, the result is joined with another DataFrame static_data_df on the\n",
    "column application_name. This means for every application in the aggregated data (joined_df), the\n",
    "corresponding details from the static_data_df are added\n",
    "\n",
    "In essence, the code segment takes streaming review data (parsed_df), groups it by\n",
    "application, computes various sentiment metrics for each application, and then enriches this\n",
    "data by joining with static application data (static_data_df) .\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['application_name', 'num_of_positive_sentiments', 'num_of_neutral_sentiments', 'num_of_negative_sentiments', 'avg_sentiment_polarity', 'avg_sentiment_subjectivity', 'category', 'rating', 'reviews', 'size', 'num_of_installs', 'price', 'age_limit', 'genres', 'version']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nExplanation: The fieldNames method retrieves the list of column names (or fields) from the\\nschema of the joined_df DataFrame. So fields_list is a Python list containing these column\\nnames.\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 8: Fetching Field Names:\n",
    "\n",
    "fields_list = joined_df.schema.fieldNames()\n",
    "# print (fields_list)\n",
    "\n",
    "'''\n",
    "Explanation: The fieldNames method retrieves the list of column names (or fields) from the\n",
    "schema of the joined_df DataFrame. So fields_list is a Python list containing these column\n",
    "names.\n",
    "So, after this line, fields_list contains all the column names of joined_df as a Python list.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<'application_name'>, Column<'num_of_positive_sentiments'>, Column<'num_of_neutral_sentiments'>, Column<'num_of_negative_sentiments'>, Column<'avg_sentiment_polarity'>, Column<'avg_sentiment_subjectivity'>, Column<'category'>, Column<'rating'>, Column<'reviews'>, Column<'size'>, Column<'num_of_installs'>, Column<'price'>, Column<'age_limit'>, Column<'genres'>, Column<'version'>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nExplanation: This line maps each column name in fields_list to its respective column in the\\nDataFrame using the F.col function. The map function is used for this purpose, and the result\\nis a list of columns, which we store in fields_as_cols.\\n\\nchatGBT:\\n\\nmap(lambda col_name: F.col(col_name), fields_list):\\n\\n    1.  map() is a Python function that applies a given function (in this case, a lambda function) to each item in a list (fields_list in this case).\\n    2.  The lambda col_name: F.col(col_name) part creates a new PySpark Column object for each column name in fields_list.\\n        In PySpark, F.col(col_name) is a function that returns a column object for a given column name, where F refers to pyspark.sql.functions.\\nSo, for each column name in fields_list, the lambda function creates an object representing that column in the DataFrame.\\n\\nlist(...): The map() function returns a map object, which is then converted to a list using list().\\n\\nExample:\\n\\nSuppose joined_df has the following schema with columns 'name', 'age', and 'city':\\n\\n    fields_list = ['name', 'age', 'city'] (a list of column names).\\n    ields_as_cols = [F.col('name'), F.col('age'), F.col('city')] (a list of column objects).\\n\\n\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 9: Mapping Field Names to Columns:\n",
    "\n",
    "fields_as_cols = list(map(lambda col_name: F.col(col_name), fields_list))\n",
    "print (fields_as_cols)\n",
    "\n",
    "'''\n",
    "Explanation: This line maps each column name in fields_list to its respective column in the\n",
    "DataFrame using the F.col function. The map function is used for this purpose, and the result\n",
    "is a list of columns, which we store in fields_as_cols.\n",
    "\n",
    "chatGBT:\n",
    "\n",
    "map(lambda col_name: F.col(col_name), fields_list):\n",
    "\n",
    "    1.  map() is a Python function that applies a given function (in this case, a lambda function) to each item in a list (fields_list in this case).\n",
    "    2.  The lambda col_name: F.col(col_name) part creates a new PySpark Column object for each column name in fields_list.\n",
    "        In PySpark, F.col(col_name) is a function that returns a column object for a given column name, where F refers to pyspark.sql.functions.\n",
    "So, for each column name in fields_list, the lambda function creates an object representing that column in the DataFrame.\n",
    "\n",
    "list(...): The map() function returns a map object, which is then converted to a list using list().\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose joined_df has the following schema with columns 'name', 'age', and 'city':\n",
    "\n",
    "    fields_list = ['name', 'age', 'city'] (a list of column names).\n",
    "    ields_as_cols = [F.col('name'), F.col('age'), F.col('city')] (a list of column objects).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nExplanation:\\n    • First, we use the withColumn method to create a new column named\\n        'to_json_struct'. This column is a struct type, combining all the columns in\\n        fields_as_cols.\\n    • Then, we use the F.to_json function to convert the struct to a JSON string. We select\\n        only this JSON string and alias it as 'value'.\\n\\n        ChatGBT:\\n            Summary:\\n                1. withColumn('to_json_struct', F.struct(fields_as_cols)):\\n                 This creates a new column to_json_struct where all the existing columns in joined_df are combined into a struct.\\n\\n                2. .select(F.to_json(F.col('to_json_struct')).alias('value')): \\n                    This converts the struct into a JSON string and renames the resulting column as 'value'.\\n                    \\nThe final DataFrame has a single column ('value') where each row contains the JSON representation of the original columns in the joined_df DataFrame.\\n\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 10: Converting Data to JSON Format:\n",
    "\n",
    "json_df = joined_df \\\n",
    ".withColumn('to_json_struct', F.struct(fields_as_cols)) \\\n",
    ".select(F.to_json(F.col('to_json_struct')).alias('value'))\n",
    "\n",
    "'''\n",
    "Explanation:\n",
    "    • First, we use the withColumn method to create a new column named\n",
    "        'to_json_struct'. This column is a struct type, combining all the columns in\n",
    "        fields_as_cols.\n",
    "    • Then, we use the F.to_json function to convert the struct to a JSON string. We select\n",
    "        only this JSON string and alias it as 'value'.\n",
    "\n",
    "        ChatGBT:\n",
    "            Summary:\n",
    "                1. withColumn('to_json_struct', F.struct(fields_as_cols)):\n",
    "                 This creates a new column to_json_struct where all the existing columns in joined_df are combined into a struct.\n",
    "\n",
    "                2. .select(F.to_json(F.col('to_json_struct')).alias('value')): \n",
    "                    This converts the struct into a JSON string and renames the resulting column as 'value'.\n",
    "                    \n",
    "The final DataFrame has a single column ('value') where each row contains the JSON representation of the original columns in the joined_df DataFrame.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/26 06:53:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/26 06:53:41 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "24/09/26 06:53:41 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "24/09/26 06:53:41 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "24/09/26 06:53:41 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "24/09/26 06:53:41 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "24/09/26 06:53:43 WARN Sender: [Producer clientId=producer-1] Got error produce response with correlation id 218 on topic-partition gps-with-reviews-0, retrying (2147483646 attempts left). Error: UNKNOWN_TOPIC_OR_PARTITION\n",
      "24/09/26 06:53:43 WARN Sender: [Producer clientId=producer-1] Received unknown topic or partition error in produce request on partition gps-with-reviews-0. The topic-partition may not exist or the user may not have Describe access to it\n",
      "24/09/26 06:53:43 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 219 : {gps-with-reviews=LEADER_NOT_AVAILABLE}\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Section 11: Writing the Stream to Kafka:\n",
    "\n",
    "query = json_df \\\n",
    ".writeStream \\\n",
    ".format('kafka') \\\n",
    ".option(\"kafka.bootstrap.servers\", \"course-kafka:9092\") \\\n",
    ".option(\"topic\", \"gps-with-reviews\") \\\n",
    ".option('checkpointLocation', 's3a://spark/checkpoints/ex6/review_calculation') \\\n",
    ".outputMode('update') \\\n",
    ".start()\n",
    "\n",
    "'''\n",
    "Explanation:\n",
    "• The writeStream method indicates we're setting up a streaming write.\n",
    "• We specify the format as 'kafka' since we're writing to a Kafka topic.\n",
    "• We provide the Kafka bootstrap server details and the target topic name.\n",
    "• The checkpoint location (checkpointLocation) is specified to allow Spark to keep\n",
    "    track of which records have been processed, ensuring fault tolerance and stream\n",
    "    resumption capabilities.\n",
    "• The outputMode is set to 'update', meaning only the rows that have changed are\n",
    "    written to the output sink.\n",
    "• Finally, start() initiates the streaming process.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 12: Waiting for Stream Termination:\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "'''\n",
    "Explanation: This line ensures the Spark application keeps running and processing the data\n",
    "until an external action stops it or it encounters an error.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nExplanation: The unpersist method removes the static_data_df DataFrame from cache. Since\\nwe called cache() on this DataFrame earlier, it's a good practice to free up memory once it's\\nno longer needed by unpersisting.\\n\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 13: Removing Cached Data:\n",
    "\n",
    "static_data_df.unpersist()\n",
    "\n",
    "'''\n",
    "Explanation: The unpersist method removes the static_data_df DataFrame from cache. Since\n",
    "we called cache() on this DataFrame earlier, it's a good practice to free up memory once it's\n",
    "no longer needed by unpersisting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExplanation: This method stops the active SparkSession, ensuring all resources are freed and\\nno further operations occur.\\n\\nSummary:\\nafter joining and aggregating the data, the code prepares the results in a JSON format and\\nstreams it to a Kafka topic. Once streaming is done, it releases the cached static data and\\nstops the Spark session.\\n\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 13: Stopping the Spark Session:\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "'''\n",
    "Explanation: This method stops the active SparkSession, ensuring all resources are freed and\n",
    "no further operations occur.\n",
    "\n",
    "Summary:\n",
    "after joining and aggregating the data, the code prepares the results in a JSON format and\n",
    "streams it to a Kafka topic. Once streaming is done, it releases the cached static data and\n",
    "stops the Spark session.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
